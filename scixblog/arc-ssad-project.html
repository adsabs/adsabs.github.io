<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="google-site-verification"
    content="q68EGUOrG2HGZ62jIdD4ILfyPYZtDK6amNJbkrav_DA"
  />

  <title>
    Identifying ADS Bibliographic Gaps Against NASA Ames Space Sciences and Astrobiology Division (ARC/SSAD)
  </title>
  <meta name="description" content="All your questions will be answered here." />

  <!-- Disqus tags -->
  <meta
    property="og:description"
    content="Identifying ADS Bibliographic Gaps Against NASA Ames Space Sciences and Astrobiology Division (ARC/SSAD)"
  />
  <meta
    property="og:image"
    content="https://ui.adsabs.harvard.edu/blog/images/blog_2021-11-12-arc-ssad_library.png"
  />

  <link
    rel="stylesheet"
    href="/help/common/css/styles.css "
  />
  <link
    rel="apple-touch-icon"
    sizes="180x180"
    href="/help/common/images/apple-touch-icon.png"
  />
  <link
    rel="manifest"
    href="/help/common/images/manifest.json"
  />
  <link
    rel="mask-icon"
    href="/help/common/images/safari-pinned-tab.svg"
    color="#5bbad5"
  />
  <meta name="theme-color" content="#ffffff" />
  <link
    rel="canonical"
    href="https://ui.adsabs.harvard.edu/scixblog/arc-ssad-project"
  />

  <link
    rel="stylesheet"
    href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.0/css/font-awesome.min.css"
  />
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      (i[r] =
        i[r] ||
        function () {
          (i[r].q = i[r].q || []).push(arguments);
        }),
        (i[r].l = 1 * new Date());
      (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m);
    })(
      window,
      document,
      'script',
      'https://www.google-analytics.com/analytics.js',
      'ga'
    );

    ga('create', 'UA-37369750-8', 'auto');
    ga('send', 'pageview');
  </script>

  <script>
    var link = document.createElement('link');
    link.rel = 'icon';
    document.head.appendChild(link);

    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
      link.href = '/help/common/images/scix/FAVICON-white-frame-32X32.png';
    } else {
      link.href = '/help/common/images/scix/FAVICON-black-frame-32X32.png';
    }
  </script>

  <style>
    body {
      visibility: hidden;
      opacity: 0;
    }
  </style>
  <noscript>
    <style>
      body {
        visibility: visible;
        opacity: 1;
      }
    </style>
  </noscript>
</head>


  <body>
    <div>
  <nav class="navbar fluid navbar-inverse" aria-label="main navigation">
    <div class="navbar-header">
      <a class="navbar-brand s-navbar-brand" href="/blog"
        ><img
          class="s-ads-icon"
          src="/help/common/images/scix/FULLCOLORLIGHT-IsotypeNoBackground.svg"
          alt="scix icon"
        />
        <b>&nbsp;SciX</b>blog
      </a>
    </div>

    <div class="navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
          <button
            type="button"
            class="btn btn-link dropdown-toggle"
            data-toggle="dropdown"
            aria-expanded="false"
          >
            <i
              class="fa fa-question-circle about-page-icon"
              aria-hidden="true"
            ></i>
            About <span class="caret"></span>
          </button>
          <ul class="dropdown-menu" role="menu">
            <li>
              <a href="/scixabout/">
                <i class="fa fa-question-circle" aria-hidden="true"></i>
                About SciX
              </a>
            </li>
            <li>
              <a href="/scixblog/">
                <i class="fa fa-newspaper-o" aria-hidden="true"></i>
                Blog
              </a>
            </li>
            <li>
              <a href="/scixhelp/">
                <i class="fa fa-info-circle" aria-hidden="true"></i>
                SciX Help Pages
              </a>
            </li>
          </ul>
        </li>
      </ul>
    </div>
  </nav>

  <div class="s-scix-background-wrapper">
    <section class="s-logo-header">
      <div>
        <a href="https://scixplorer.org/">
          <img
            src="/help/common/images/scix/FULLCOLORLIGHT-IsotypeNoBackground.svg"
            alt="The SciX Logo"
          />
          Science Explorer
        </a>
      </div>
    </section>
  </div>
</div>


    <div class="smart-container row">
      <div class="col-xs-4 col-md-2 rss-container">
        <a href="/help/common/feed.xml"
          ><i class="fa fa-rss"></i> RSS Feed</a
        >
      </div>
      <div class="blog-links col-xs-8 col-md-4">
        <a href="/scixblog">all blog posts</a> |
        <a href="/scixblog/news">news</a> |
        <a href="/scixblog/general">general</a> |
        <a href="/scixblog/technical-posts">technical</a>
      </div>

      <div class="search-bar-container col-xs-12 col-md-6">
        <div class="pull-right"><script>
  (function () {
    var cx = '016381517138775266827:j_b8wqloe18';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
      </div>
    </div>
    <div class="page-content smart-container"><div class="row row-offcanvas row-offcanvas-left">
  <div
    class="post main-content col-md-8 col-md-offset-2 col-sm-10 col-sm-offset-1"
  >
    <header class="post-header">
      
      <h1 class="post-title">Identifying ADS Bibliographic Gaps Against NASA Ames Space Sciences and Astrobiology Division (ARC/SSAD)</h1>
      <h3 class="post-author">



    
        
            Jennifer Koch
              
        
    


    <b>(ADS)</b>

</h3>
      <h3 class="post-date">12 Nov 2021</h3>

      

      <hr />

      
    </header>

    

    <article class="post-content">
<p>Recently NASA has asked the Astrophysics Data System to develop a plan to expand its service from Astrophysics to cover all five scientific disciplines supported by NASA‚Äôs Science Mission Directorate (Heliophysics, Planetary Science, Astrophysics, Earth Science, Biophysics). The initial phase of this expansion, into Planetary Science and Heliophysics, has now been approved and funded (<a href="https://ui.adsabs.harvard.edu/abs/2021AAS...23813203A/abstract">Accomazzi 2021</a>). Over the past year we developed a census to ensure research areas such as Space Science, Astrobiology, Aeronomy and Solar Physics are properly accounted for and represented in our database. The ultimate goal of this effort is to provide the same level of support for these disciplines as ADS currently provides for Astrophysics: current and accurate coverage of both refereed and gray literature, preprints, data and software. We expect that enhanced search capabilities will be developed in due time through collaborations with partners and stakeholders.</p>

<p>The project described in this blog is a collaboration with <a href="https://www.nasa.gov/ames/spacescience-and-astrobiology/overview">NASA Ames Space Sciences and Astrobiology Division (ARC/SSAD)</a> to assess the bibliographic holdings of ARC/SSAD, identify those that have already been included in ADS‚Äôs holdings, flag those that are not yet included, and finally curate records and ingest those missing into ADS. This is one example of ADS‚Äôs goal to form collaborations with partners and expand access to scientific data and literature. The results of this project will be a first indication of how well the ADS covers the literature relevant for this type of research.</p>

<p>As a recent addition to the ADS Team supporting curation efforts and assisting in collection management, this appealed to my interests as I have recently begun honing my Python skills and learning new tools such as Jupyter Notebook and OpenRefine. Jupyter Notebook is especially useful for new Python users because it helps break up scripts into more manageable blocks (cells) and notes, findings and documentation can be included along the way.</p>

<p>In this blog post I will outline the goals I established, the steps I took to accomplish them, and lessons learned. To accomplish this project, I used a combination of my own knowledge and expertise, read API documentation, searched online for solutions as needed, and collaborated with team members to debug and learn the ins and outs of the ADS API.</p>

<h3 id="project-outline-and-goals">Project Outline and Goals</h3>

<p>The source data used in this project was an Excel spreadsheet provided of ARC/SSAD‚Äôs bibliographic holdings (with metadata for authors, ‚Äòorg code‚Äô as a unique identifier of NASA‚Äôs organizational structure, title, journal information, and DOI if available), which was split up into three sheets by Branch (Astrophysics Branch, the Planetary Systems Branch, and the Exobiology Branch). The data provided was obtained from NASA‚Äôs Astrobiology Habitable Environments Database (AHED).</p>

<div class="text-center">
    <img class="img-thumbnail" alt="Sample image of original excel file, showing items in three tabs for the ARC/SSAD Branches" src="/blog/images/blog_2021-11-12-ahed_excel.png" />
<em>Sample image of original Excel file, showing items in three tabs for the ARC/SSAD Branches</em>
</div>
<p><br /></p>

<p>The main overall goal was to check if the publication is present in the ADS database, and if so, match it with an ADS bibcode. In order to accomplish this, I used the ADS API to match based on DOIs (for those provided), then reference strings, and then fill in the rest by title. I split up my overall goal into four major tasks, and for each of these phases I created a new Jupyter Notebook and outlined the steps.</p>
<ol>
  <li><a href="#match-by-doi">Task 1: Match ARC/SSAD to ADS Items by DOI</a></li>
  <li><a href="#match-by-ref">Task 2: Match ARC/SSAD to ADS Items by Reference Strings</a></li>
  <li><a href="#match-by-title">Task 3: Match ARC/SSAD to ADS Items by Title</a></li>
  <li><a href="#ads-libs">Task 4: Curate missing items and create ADS Libraries</a></li>
</ol>

<h2 id="task-1-match-arcssad-to-ads-items-by-doi"><a name="match-by-doi">Task 1: Match ARC/SSAD to ADS Items by DOI</a></h2>

<p>Code snippets in this section are taken from <a href="https://github.com/jrkoch127/ahed_ads_project/blob/main/SSAD-1-DOIs%20API.ipynb">ARC/SSAD Project Notebook 1</a>.</p>

<p>My first goal is to match items in the ARC/SSAD spreadsheet to ADS records by DOI. I first created a version of the ARC/SSAD spreadsheet that has a column ‚Äúbibcode‚Äù added to the right of the DOI column. For those publications we are able to match to ADS records, this column will list these bibcodes, otherwise ‚ÄúNA‚Äù.</p>

<p><b>Task 1 Outline:</b></p>
<ul>
  <li>Step 1.1: Data Cleanup and Prep - combine all the Excel sheets to one data set</li>
  <li>Step 1.2: Isolate/create list of DOIs</li>
  <li>Step 1.3: API Connection &amp; Query</li>
  <li>Step 1.4: Match list of bibcodes to original data set</li>
</ul>

<p><b>Step 1.1: Data Cleanup and Prep</b></p>

<p>After opening up a Jupyter notebook, I began by loading the Excel spreadsheets and merging them as one comprehensive data frame. I decided this would be easier to handle and more efficient for obtaining results.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
import pandas as pd

# Read Excel sheets
astro_pubs = pd.read_excel("/SpaceScienceAndAstrobiologyDivision.xlsx", sheet_name=0)
planet_pubs = pd.read_excel("AHED/SpaceScienceAndAstrobiologyDivision.xlsx", sheet_name=1)
exo_pubs = pd.read_excel("AHED/SpaceScienceAndAstrobiologyDivision.xlsx", sheet_name=2)

# Combine the excel sheets into one new/big data frame
ahed_pubs = pd.concat([astro_pubs, planet_pubs, exo_pubs], axis='index', ignore_index=True)

# Export to new excel file
ahed_pubs.to_excel("AHED/ahed_pubs.xlsx", index=False)
</code></pre></div></div>

<p>After creating a single data frame, I used OpenRefine to clean up, transform, and normalize the data. <a href="https://openrefine.org/">OpenRefine</a> is an open-source application for working with noisy data. I recently attended training at a <a href="https://datascience.si.edu/carpentries">Smithsonian Data Carpentries Workshop</a> where I learned about OpenRefine, its uses, and benefits for data cleanup.</p>

<p>Using OpenRefine, I was able to transform the journal data, normalizing publication titles, volume and issue numbers and standardizing formatting. This also helped me discover additional DOIs in the Journal field. In addition, I made transformations such as trimming whitespace, fixing typos, simplifying column headers, and removing duplicate entries. The transformation and deduplication process narrowed down the ARC/SSAD paper list from 892 to 797 items.</p>

<p><b>Step 1.2: Isolate the list of DOIs</b></p>

<p>With my cleaned up data, I imported the new Excel file and isolated all the existing DOIs to prep for querying them in the ADS API.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
# Import new Excel sheet, 'ahed_pubs_refined'
ahed_pubs_refined = pd.read_excel("AHED/ahed_pubs_refined.xlsx")

# Isolate the DOIs and drop all the papers that have no DOIs (drop null values)
ahed_dois = ahed_pubs_refined['DOI']
ahed_dois.dropna(inplace = True)

# Convert it from a data frame to a list
ahed_doi_list = ahed_dois.to_list()
print("Original paper list has", len(ahed_doi_list), "DOIs to search.")
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original paper list has 177 DOIs to search.
</code></pre></div></div>

<p><b>Step 1.3: API Connection &amp; Query</b></p>

<p>Ready to search the ADS API with 177 DOIs, I established the API connection and queried my DOIs, returning the bibcodes and DOIs matched to ADS‚Äô holdings. At first, I established a basic DOI input query, where I joined all 177 DOIs in a single string, joined by ‚ÄòOR‚Äô so that the ADS API would search them all at once. This worked, however it was not the most efficient due to a character limit in the query. After some iteration, I was able to formulate a loop through my DOI list in batches of 20 and append the response bibcodes and DOIs to a new list (‚Äòdata = []‚Äô).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
import requests
import json

# --- API REQUEST --- 
token = "&lt;my token here&gt;"
url = "https://api.adsabs.harvard.edu/v1/search/query?"

data=[]

for i in range(0, len(ahed_doi_list), 20):
    chunk = ahed_doi_list[i:i + 20]
    tagged = ['doi:' + d for d in chunk]
    query = ' OR '.join(tagged)
    
    params = {"q":query,"fl":"doi,bibcode","rows":200}
    headers = {'Authorization': 'Bearer ' + token}
    response = requests.get(url, params=params, headers=headers)

    from_solr = response.json()
    if (from_solr.get('response')):
        num_docs = from_solr['response'].get('numFound', 0)
        if num_docs &gt; 0:
            for doc in from_solr['response']['docs']:
                data.append((doc['bibcode'],doc['doi'][0]))

# Create new data frame of response data
dois_matched = pd.DataFrame(data, columns = ['bibcode','DOI'])

# Export new excel sheet
dois_matched.to_excel("AHED/dois_matched.xlsx",
                  index=False)
</code></pre></div></div>

<p><b>Step 1.4: Match Bibcode/DOI Response Data to Original Paper List</b></p>

<p>After the API connection successfully matched 155 existing bibcodes to the DOIs queried, my new step was to join these bibcodes on the DOIs in the ARC/SSAD paper list. I joined the new data set (consisting of two columns, ‚ÄòDOI‚Äô and ‚ÄòBIBCODE‚Äô) to the old as a left join on ‚ÄòDOI‚Äô.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
# Merge/Join new table to original, joined on 'DOI'
merged = ahed_pubs_refined.merge(dois_matched, on='DOI', how='left')

# Export merged data
merged.to_excel("AHED/dois_matched.xlsx",
                  index=False)
</code></pre></div></div>

<h2 id="task-2-match-arcssad-to-ads-items-by-reference-strings"><a name="match-by-ref">Task 2: Match ARC/SSAD to ADS Items by Reference Strings</a></h2>

<p>Code snippets in this section are taken from <a href="https://github.com/jrkoch127/ahed_ads_project/blob/main/AHED-2-Ref%20API.ipynb">ARC/SSAD Project Notebook 2</a>.</p>

<p>My next goal was to match papers not matched by DOI in the previous task using their reference strings instead via the ADS Reference Service. The <a href="https://github.com/adsabs/reference_service">ADS Reference Service</a> is an API endpoint that can take a query string of authors and/or journal info (publication name, volume, issue, year) and return the bibcode if matched to an existing ADS record.</p>

<p><b>Task 2 Outline:</b></p>
<ul>
  <li>Step 2.1: Format file of papers into reference strings</li>
  <li>Step 2.2: Query the Reference API with reference strings, return bibcodes</li>
  <li>Step 2.3: Match the bibcodes back to the paper list</li>
</ul>

<p><b>Step 2.1: Format Reference List</b></p>

<p>First I needed to prep the reference strings. In Task 1, I had transformed the data in OpenRefine to normalize the journal titles, volume numbers, and issue/id numbers. The Reference Service takes strings in the following format: [authors],[publication year],[journal name, vol, issue numbers]. For example, ‚ÄúRoser, J. E., Ricca, A., and Allamandola, L. J., 2014, ApJ, 783, 97‚Äù would be a typical reference string that the service can query. So I started to formulate these reference strings by joining these metadata fields together into a new column and exporting the column/list to a text file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
import pandas as pd
import numpy as np

# Open my excel sheet as a data frame
df = pd.read_excel("AHED/dois_matched.xlsx")

# String together the fields into single reference strings (Authors, Year, Journal)
df['REFS'] = df['AUTHORS'].astype(str) + ', ' + df['YEAR'].astype(str) + ', ' + df['JOURNAL'].astype(str)

# Grab only rows where DOI is null
dt = df[df['DOI'].isna()]

# Export my reference strings to text file
dt['REFS'].to_csv("AHED/ref_list.txt", index=False, header=False, sep='\t')
</code></pre></div></div>

<p><b>Step 2.2: Connect to Reference Service API</b></p>

<p>The next step was to input my reference list to the API, and return the matching bibcodes.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
import sys, os, io
import requests
import argparse
import json

# ADS Prod API Token
token = '&lt;my token here&gt;'
domain = 'https://api.adsabs.harvard.edu/v1/'

## REFERENCE SERVICE ##

# --- Function to read my reference strings file and make a list called 'references'
def read_file(filename):

    references = []
    with open(filename, "r") as f:
        for line in f:
            references.append(line)
    return references

# --- Function to connect to Reference Service API, querying my 'references' list
def resolve(references):
    
    payload = {'reference': references}

    response = requests.post(
        url = domain + 'reference/text',
        headers = {'Authorization': 'Bearer ' + token,
                 'Content-Type': 'application/json',
                 'Accept':'application/json'},
        data = json.dumps(payload)
    )
    
    if response.status_code == 200:
        return json.loads(response.content)['resolved'], 200
    else:
        print('From reference status_code is ', response.status_code)
    return None, response.status_code
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
# Read my reference strings file
references = read_file("AHED/ref_list.txt")
references = [ref.replace('\n','') for ref in references]

# Resolve my references, results in 'total results' list
total_results = []

for i in range(0, len(references), 16):
    results, status = resolve(references[i:i+16])
    if results:
        total_results += results

# Method to count how many total bibcodes were matched
bibcodes = []
for record in total_results:
    if record['bibcode']!='...................':
        bibcodes.append(record['bibcode'])

print('Matched',len(bibcodes),'bibcodes')
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Matched 397 bibcodes
</code></pre></div></div>

<p><b>Step 2.3: Match Bibcode/Reference Response Data to Original Paper List</b></p>

<p>After the API successfully found 397 bibcodes from the rest of the paper list (~650 papers), my next step was to match these back to the original ARC/SSAD paper list and include as items matched thus far.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
# Convert my reference results to a data frame and drop null values
ref_results = pd.DataFrame(total_results)
ref_results = ref_results.replace('...................', np.nan)
ref_results = ref_results.dropna(subset=['bibcode'])

# Merge my new ref service results with my original paper list, join by the refstrings
merged = pd.merge(df, ref_results, how='left', left_on='REFS', right_on='refstring')
merged

# Combine bibcode columns, bringing new bibcode column over to the existing bibcode column
merged['BIBCODE'] = merged['BIBCODE'].fillna(merged['bibcode'])

# Cleanup; drop unneeded columns
merged = merged.drop('refstring',axis=1)
merged = merged.drop('REFS',axis=1)
merged = merged.drop('bibcode',axis=1)
merged = merged.drop('score',axis=1)
merged = merged.drop('comment',axis=1)

# Clean up nulls
merged = merged.replace(np.nan,'NA')

# Export merged data to new excel file
merged.to_excel("AHED/refs_matched.xlsx", index=False)
</code></pre></div></div>
<p>Now at a running total of approx 550 items matched, my last goal was to match any additional items I could find by Title.</p>

<h2 id="task-3-match-arcssad-to-ads-items-by-title"><a name="match-by-title">Task 3: Match ARC/SSAD to ADS Items by Title</a></h2>

<p>Code snippets in this section are taken from <a href="https://github.com/jrkoch127/ahed_ads_project/blob/main/AHED-3-Titles%20API.ipynb">ARC/SSAD Project Notebook 3</a>.</p>

<p>My next goal was to match additional papers (without DOIs matched, nor reference strings matched) this time by Title via the ADS API. This Title matching task was mostly for publications that had incomplete metadata (like missing volume, issue, page numbers, or incorrect journal names).</p>

<p>For this task, I chose to include the publication year for most accurate results. Therefore, a typical reference string for this task would be <code class="language-plaintext highlighter-rouge">(title: "1-2.4 Œºm Near-IR Spectrum of the Giant Planet Œ≤ Pictoris b Obtained with the Gemini Planet Imager" AND year:2017)</code>. This can be challenging for the ADS API if the Title is not spelled correctly or has special characters. This also proved challenging where the year metadata didn‚Äôt match. Therefore, I first formatted a file of these strings (Title + Year) and queried the ADS API to match any additional existing bibcodes. After that, I took a final list of unmatched items, queried the API by individual Titles, and assessed the results manually.</p>

<p><b>Task 3 Outline:</b></p>
<ul>
  <li>Step 3.1: Format titles to query the ADS API</li>
  <li>Step 3.2: Query the ADS API with titles, return bibcodes</li>
  <li>Step 3.3: Match the bibcodes back to the paper list</li>
</ul>

<p><b>Step 3.1: Format Titles List</b></p>

<p>From here I grabbed my running list of papers (‚Äòrefs_matched‚Äô from Task 2), isolated the rows that have no bibcode yet, and formulated query strings of ‚ÄòTitle + Year‚Äô.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
import pandas as pd
import numpy as np

# Open my excel sheet as a data frame
df = pd.read_excel("AHED/refs_matched.xlsx")

# Grab only rows where bibcode is null
dt = df[df['BIBCODE'].isna()]

# Create title &amp; year query strings
dt['QUERY'] = ('(title: "' + dt['TITLE'].astype(str) + '" AND year:' + dt['YEAR'].astype(str) + ')')

# Format query list
titles = dt['QUERY'].to_list()
</code></pre></div></div>

<p><b>Step 3.2: API Connection &amp; Query</b></p>

<p>With my Titles ready to query, I set up the API connection, and queried the list in chunks of 25 since there were ~250 items to input, and the API could only take so many at a time. From the ADS API response, I created a new data frame with the bibcodes and titles returned.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
# This loops through the titles list in chunks of 25 titles, querying the API, 
# returning bibcodes and titles matched, and then appending the results as a data frame.

import requests
import json

# --- API REQUEST --- 
token = "&lt;my token here&gt;"
url = "https://api.adsabs.harvard.edu/v1/search/query?"

data=[]

for i in range(0, len(titles), 25):
    chunk = titles[i:i + 25]
    tagged = [t for t in chunk]
    query = ' OR '.join(tagged)
    
    params = {"q":query,"fl":"title,bibcode","rows":200}
    headers = {'Authorization': 'Bearer ' + token}
    response = requests.get(url, params=params, headers=headers)

    from_solr = response.json()
    if (from_solr.get('response')):
        num_docs = from_solr['response'].get('numFound', 0)
        if num_docs &gt; 0:
            for doc in from_solr['response']['docs']:
                data.append((doc['bibcode'],doc['title'][0]))

titles_matched = pd.DataFrame(data, columns = ['bibcode','TITLE'])

</code></pre></div></div>
<p><b>Step 3.3: Match Bibcode/Title Response Data to Original Paper List</b></p>

<p>Finally, I was able to merge the new titles &amp; bibcodes to my running list of matches (‚Äòrefs_matched‚Äô).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
# Merge/Join new table to original, joined on 'TITLE'
merged = df.merge(titles_matched, on='TITLE', how='left')

# Combine bibcode columns
merged['BIBCODE'] = merged['BIBCODE'].fillna(merged['bibcode'])
merged = merged.drop('bibcode',axis=1)

# Clean up nulls
merged = merged.replace(np.nan,'NA')

# Export merged data to new excel file
merged.to_excel("AHED/final_matched_2.xlsx", index=False)
</code></pre></div></div>
<p>With this merge, my total came up to about 692 items matched out of a potential 797. After some analysis of what was left unmatched, I found quite a few discrepancies in the ‚ÄòYear‚Äô metadata, as well as ‚ÄòTitle‚Äô mismatches (i.e. typos in the metadata, titles changed during publication, etc.) so when I searched again by Title alone, I ended up finding an additional ~40 or so papers that matched ADS holdings, bringing my final total to 731 items.</p>

<h2 id="task-4-curate-missing-items-and-create-ads-libraries"><a name="ads-libs">Task 4: Curate missing items and create ADS Libraries</a></h2>

<p>Code snippets in this section are taken from <a href="https://github.com/jrkoch127/ahed_ads_project/blob/main/AHED-4-Libraries.ipynb">ARC/SSAD Project Notebook 4</a>.</p>

<p>After successfully identifying as many bibcodes as I could match between ARC/SSAD and ADS, I grabbed my full list of bibcodes and I made another ADS Library of them, which can be accessed <a href="https://ui.adsabs.harvard.edu/user/libraries/1gM2Y7nVSv-POu2lanjJ6g">here</a>.</p>

<p>My final task for this project was to identify, locate, and curate the last ~70 ARC/SSAD records missing from the ADS holdings. This was a manual process of searching the web with the metadata provided, locating the applicable DOI, and curating ADS records from there. I was able to identify and locate approximately 54 publications, plus records for individual chapters for two I identified as books. As a result, the ADS team ingested these new records and I again created a library of those (102 total), which can be found <a href="https://ui.adsabs.harvard.edu/user/libraries/HkCPGwYhSSWpzvJW_gxd3w">here</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
import requests
import json

# --- API REQUEST --- 
token = "&lt;my token here&gt;"
url = "https://api.adsabs.harvard.edu/v1/biblib/libraries"
    
data = { 
    "name":"ARC/SSAD Library",
    "description":"Library of records ADS matches of ARC/SSAD holdings",
    "public": True,
    "bibcode": bibs
}
headers = {'Authorization': 'Bearer ' + token}
response = requests.post(url, data=json.dumps(data), headers=headers)

print(response.status_code)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>200
</code></pre></div></div>

<h2 id="lessons-learned">Lessons Learned</h2>
<ul>
  <li>Using OpenRefine to do the bulk of your data prep and cleanup was a great advantage</li>
  <li>Try to save fewer files; exporting/importing new CSVs/files with small tweaks can disrupt the flow of the script and create a bunch of extra files that will clutter your documents</li>
  <li>Look for more quantitative ways of measuring my progress - how many bibcodes matched, counting duplicates vs non-duplicates</li>
  <li>Look for ways to measure accuracy of bibcode matching - use ‚Äòscore‚Äô field in ADS API</li>
  <li>Practice using loops and functions for greater efficiency</li>
</ul>

<h2 id="results-and-conclusions">Results and Conclusions</h2>

<p>Overall, I was able to match the majority (about 92%) of publications from the ARC/SSAD list to bibcodes in ADS. Only a handful of records needed to be further searched and curated for inclusion in the collections. The ADS API was impressively accurate in returning appropriate bibcodes with each task I completed. There were even times when I thought ADS had matched the wrong bibcode, until further investigation showed me that the provided metadata was inconsistent or incomplete (title, journal, or year differences). As such, this project also illustrated the general challenge involved with matching reference data to existing publications. The reference matching software has to be flexible enough to deal with small inaccuracies, but not to be too lenient (which would generate a high probability of false positives). The ADS Reference Service does well in this regard, and includes a ‚Äòscore‚Äô quality that can be used to assess the accuracy of matches. This project was an excellent way to put my Python, data, and librarianship skills into practice, and to get acquainted with all of ADS‚Äô API services.</p>
</article>
  </div>
</div>
<div class="row">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'adsabsblog';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
      var dsq = document.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (
        document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]
      ).appendChild(dsq);
    })();
  </script>
  <noscript
    >Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript" rel="nofollow"
      >comments powered by Disqus.</a
    ></noscript
  >
</div>
</div>

    <footer>
  <div class="__footer_wrapper">
    <div class="__footer_brand">
      &copy; The SAO Astrophysics Data System
      <div class="__footer_brand_extra">
        <p>
          <i class="fa fa-envelope" aria-hidden="true"></i>
          help[at]scixplorer.org
        </p>
        <p>
          SciX is a project created by the Astrophysics Data System (ADS), which
          is operated by the Smithsonian Astrophysical Observatory under NASA
          Cooperative Agreement <em>80NSSC21M0056</em>
        </p>
      </div>
      <div class="__footer_brand_logos">
        <a href="http://www.si.edu" target="_blank" rel="noreferrer noopener">
          <img
            id="smithsonian-logo"
            src="/help/common/images/smithsonian.svg"
            alt="Smithsonian logo"
          />
        </a>
        <a
          href="https://www.cfa.harvard.edu/"
          target="_blank"
          rel="noreferrer noopener"
        >
          <img
            src="/help/common/images/cfa.png"
            alt="Harvard Center for Astrophysics logo"
            id="cfa-logo"
          />
        </a>
        <a href="http://www.nasa.gov" target="_blank" rel="noreferrer noopener">
          <img
            src="/help/common/images/nasa-partner.svg"
            alt="NASA logo"
            id="nasa-logo"
          />
        </a>
      </div>
      <div class="footer-disclaimer">
        *The material contained in this document is based upon work supported by
        a National Aeronautics and Space Administration (NASA) grant or
        cooperative agreement. Any opinions, findings, conclusions or
        recommendations expressed in this material are those of the author and
        do not necessarily reflect the views of NASA.
      </div>
    </div>
    <div class="__footer_list">
      <div class="__footer_list_title">Resources</div>
      <ul class="__footer_links">
        <li>
          <a href="/scixabout/">
            <i class="fa fa-question-circle" aria-hidden="true"></i> About SciX
          </a>
        </li>
        <li>
          <a href="/scixhelp/">
            <i class="fa fa-info-circle" aria-hidden="true"></i> SciX Help
          </a>
        </li>
        <li>
          <a href="/help/whats_new/">
            <i class="fa fa-bullhorn" aria-hidden="true"></i> What's New
          </a>
        </li>
        <li>
          <a href="/scixabout/careers/">
            <i class="fa fa-group" aria-hidden="true"></i> Careers@SciX
          </a>
        </li>
        <li>
          <a href="/help/accessibility/">
            <i class="fa fa-universal-access" aria-hidden="true"></i>
            Accessibility
          </a>
        </li>
      </ul>
    </div>
    <div class="__footer_list">
      <div class="__footer_list_title">Social</div>
      <ul class="__footer_links">
        <li>
          <a
            href="//twitter.com/scixcommunity"
            target="_blank"
            rel="noreferrer noopener"
          >
            <i class="fa fa-twitter" aria-hidden="true"></i> @scixcommunity
          </a>
        </li>
        <li>
          <a href="/blog/">
            <i class="fa fa-newspaper-o" aria-hidden="true"></i> ADS Blog
          </a>
        </li>
      </ul>
    </div>
    <div class="__footer_list">
      <div class="__footer_list_title">Project</div>
      <ul class="__footer_links">
        <li>
          <a href="/help/privacy/">Privacy Policy</a>
        </li>
        <li>
          <a href="/help/terms">Terms of Use</a>
        </li>
        <li>
          <a
            href="http://www.cfa.harvard.edu/sao"
            target="_blank"
            rel="noreferrer noopener"
            >Smithsonian Astrophysical Observatory</a
          >
        </li>
        <li>
          <a href="http://www.si.edu" target="_blank" rel="noreferrer noopener"
            >Smithsonian Institution</a
          >
        </li>
        <li>
          <a
            href="http://www.nasa.gov"
            target="_blank"
            rel="noreferrer noopener"
            >NASA</a
          >
        </li>
      </ul>
    </div>
  </div>
</footer>

    <div id="darkSwitch" class="darkmode-toggle" title="Turn on dark mode">
      üåì
    </div>
    <script src="/help/common/js/dark-mode-switch.js"></script>
  </body>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.js"></script>
  <script
    src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"
    integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS"
    crossorigin="anonymous"
  ></script>

  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      (i[r] =
        i[r] ||
        function () {
          (i[r].q = i[r].q || []).push(arguments);
        }),
        (i[r].l = 1 * new Date());
      (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m);
    })(
      window,
      document,
      'script',
      '//www.google-analytics.com/analytics.js',
      'ga'
    );

    ga('create', 'UA-37369750-8', 'auto');
    ga('send', 'pageview');
  </script>
</html>
